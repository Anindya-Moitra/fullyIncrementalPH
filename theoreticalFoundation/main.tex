\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{times}
\usepackage{cite}
\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsfonts}
%\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{url}
\usepackage{amsthm}
% \usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\usepackage{color}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{relsize}


\begin{document}

\title{Incremental Matrix Reduction}

\maketitle

\section{The Problem}

This document addresses one of the primary challenges of computing persistent homology on streaming 
data by a fully incremental approach.  As a data stream is a potentially infinite sequence of data 
objects, the entire stream cannot be stored in the memory typically available to a computer.  
Therefore, the computation of persistent homology on streaming data requires an incremental 
approach.  A couple of computational models for applying persistent homology on data streams are 
being developed as two separate projects that involve partially incremental approaches.  In 
particular, consistent with the standard computational paradigm \cite{silva-13} for processing 
data streams, each of those two models consists of two principal components: (i) \emph{online}, and 
(ii) \emph{offline}.  However, yet another (\emph{i.e.}, a third) computational model can be 
developed by a \emph{fully online} or \emph{fully incremental} approach.


A key requirement for developing such a fully incremental model for persistent homology would be 
the ability to perform the \emph{Gaussian elimination} (also called the \emph{reduction}) of the 
boundary matrix \cite{edelsbrunner-00, zomorodian-05} by an incremental algorithm.  The Gaussian 
elimination step is performed during the offline component (\emph{i.e.}, as a \emph{batch 
processing} mechanism) in the previous two models for computing persistent homology on streaming 
data.  This document attempts to develop the theoretical foundation for performing the 
\emph{Gaussian elimination} by an incremental algorithm.


The standard algorithm \cite{edelsbrunner-00, zomorodian-05} \emph{reduces} the boundary matrix 
$\partial$ of a filtration \cite{zomorodian-10} to the column-echelon form $R$.  Usually, the 
entire boundary matrix or a simplified data structure thereof is processed in the memory while the 
standard algorithm is executed.  This approach is not desirable when computing persistent homology 
on streaming data by a fully incremental mechanism.  When working with data streams processed by a 
fully incremental model, one would want to add a simplex $\sigma$ to the already reduced matrix $R$ 
without having to recompute the reduction of other columns due to the addition of $\sigma$.  
Ideally, only the column of $\sigma$ should be reduced as the simplex $\sigma$ is added to the 
filtration.

It is worth mentioning that while computing persistent homology on data streams is one of the 
primary target applications of the incremental Gaussian elimination algorithm, it will have other 
important applications as well.  For example, due to the large size of the \emph{complex} 
constructed on a point cloud, the dimension of the boundary matrix increases exponentially with the 
number of data objects on which persistent homology is being computed.  As a result, reducing the 
boundary matrix by a batch processing algorithm becomes prohibitive even for `static' (\emph{i.e.}, 
non-streaming) data sets of moderate size (such as, data sets with up to a few thousands of 
objects, depending on the memory available to the computer).  The incremental algorithm developed 
by this project will help in the Gaussian elimination of the boundary matrix where this is 
not possible by batch processing mechanisms due to the size of the matrix.


Kerber \textit{et al.} \cite{kerber-18} introduced a streaming algorithm for reducing the boundary 
matrix based on optimized versions of the standard algorithm \cite{edelsbrunner-00, 
zomorodian-05}.  However, their algorithm assumes that the entire data set or filtration is 
available on disk.  Therefore, the total ordering of the simplices is predetermined.  The next 
simplex added to $R$ has a higher weight than any of the previously added simplices.


In a real streaming application, the entire point cloud or filtration is not available at any 
time.  Hence, every time a new simplex $\sigma$ is added to the filtration, the indices of those 
simplices that have weights higher than that of $\sigma$ are incremented by one.  In other words, 
in a real streaming environment, new simplices do not arrive from a sequence of simplices sorted 
according to their increasing weights.  Therefore, the algorithm of \cite{kerber-18} can not be 
applied to real-world streaming applications.



In this document, we prove that a streaming or incremental version of the standard algorithm 
exists.  In particular, we prove that even when a simplex $\sigma$ that changes the indices of 
other simplices is added to an already reduced matrix $R$, it is possible to only reduce the column 
of $\sigma$ without recomputing the reduction of other columns.



\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}