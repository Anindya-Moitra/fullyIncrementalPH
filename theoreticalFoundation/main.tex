\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{times}
\usepackage{cite}
\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsfonts}
%\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{url}
\usepackage{amsthm}
% \usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\usepackage{color}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{relsize}


\author{Anindya Moitra}

\title{Incremental Matrix Reduction}
\date{\vspace{-5ex}}


\begin{document}
	

\maketitle

\section{The Background}

This document addresses one of the primary challenges of computing persistent homology on streaming 
data by a fully incremental approach.  As a data stream is a potentially infinite sequence of data 
objects, the entire stream cannot be stored in the memory typically available to a computer.  
Therefore, the computation of persistent homology on streaming data requires an incremental 
approach.  A couple of computational models for applying persistent homology on data streams are 
being developed as two separate projects that involve partially incremental approaches.  In 
particular, consistent with the standard computational paradigm \cite{silva-13} for processing 
data streams, each of those two models consists of two principal components: (i) \emph{online}, and 
(ii) \emph{offline}.  However, yet another (\emph{i.e.}, a third) computational model is developed 
by a \emph{fully online} or \emph{fully incremental} approach in this project.


A key requirement for developing such a fully incremental model for persistent homology is the 
ability to perform the \emph{Gaussian elimination} (also called the \emph{reduction}) of the 
boundary matrix \cite{edelsbrunner-00, zomorodian-05} by an incremental algorithm.  The Gaussian 
elimination step is performed during the offline component (\emph{i.e.}, as a \emph{batch 
processing} mechanism) in the previous two models for computing persistent homology on streaming 
data.  This document develops the theoretical foundation for performing the \emph{Gaussian 
elimination} by an incremental algorithm.


It is worth mentioning that while computing persistent homology on data streams is one of the 
primary target applications of the incremental Gaussian elimination algorithm, it will have other 
important applications as well.  For example, due to the large size of the \emph{complex} 
constructed on a point cloud, the dimension of the boundary matrix increases exponentially with the 
number of data objects on which persistent homology is being computed.  As a result, reducing the 
boundary matrix by a batch processing algorithm becomes prohibitive even for `static' (\emph{i.e.}, 
non-streaming) data sets of moderate size (such as, data sets with up to a few thousands of 
objects, depending on the memory available to the computer).  The incremental algorithm developed 
by this project will help in the Gaussian elimination of the boundary matrix where this is 
not possible by batch processing mechanisms due to the size of the matrix.




\section{The Problem}


The standard algorithm \cite{edelsbrunner-00, zomorodian-05}, as described in 
\cite{edelsbrunner-10, otter-17, chen-11, kerber-18} among others, computes the persistence of a 
filtration \cite{zomorodian-10} by \emph{reducing} its boundary matrix $\partial$ to a 
column-echelon form $R$.  Usually, the entire boundary matrix or a simplified data structure 
thereof is processed in the memory while the algorithm is executed.  This approach is not desirable 
when computing persistent homology on streaming data by a fully incremental mechanism.  When 
working with data streams processed by a fully incremental model, one would want to add a simplex 
$\sigma$ to the already reduced matrix $R$ without having to recompute the reduction of all other 
columns due to the addition of $\sigma$.


Kerber \emph{et al.} \cite{kerber-18} introduced a streaming algorithm for reducing the boundary 
matrix based on optimized versions of the standard algorithm \cite{edelsbrunner-00, 
zomorodian-05}.  However, their algorithm assumes that the entire data set or filtration is 
available on disk.  Therefore, the total ordering of all the simplices in the filtration is known 
\emph{a priori}.  The next simplex added to $R$ has a higher weight than any of the previously 
added simplices.


In a real streaming application, the entire point cloud or filtration is not available at any 
time.  Hence, every time a new simplex $\sigma$ is added to the filtration, the indices of those 
simplices that have weights higher than that of $\sigma$ are incremented by one.  In other words, 
in a real streaming environment, the column corresponding to a new simplex does not necessarily get 
added to the right of $R$.  Therefore, the algorithm of \cite{kerber-18} can not be applied to 
real-world streaming applications.


In the next section, we provide two algorithms to incrementally reduce a boundary matrix after the 
addition of simplices.  The goal of both of the algorithms is to compute the updated reduction of 
the boundary matrix $\partial$ when new columns are added to $R$, the existing reduction of 
$\partial$.



\section{Addition of Simplices: Two Solutions}

\subsection{Algorithm 1}

Let us assume that $R$ is the reduction of a boundary matrix $\partial$ associated to a filtration 
$K$ that has $n$ simplices.  When a new simplex $\sigma$ needs to be added to $R$, we compute the 
index $j$ and the unreduced column $\partial_j$ of $\sigma$ with respect to the total ordering of 
the filtration.  $\partial_j$ specifies the facets\footnote{A facet is a co-dimension one face of a 
simplex.} of $\sigma$.

We then apply an optimization technique called \emph{compression} \cite{kerber-18} on the column 
$\partial_j$: scan through the non-zero entries of $\partial_j$; If a row index $i$ corresponds to 
a negative simplex (\emph{i.e.}, if the $i$-th column of $R$ is not zero), remove the non-zero entry
from the column $\partial_j$.  We then insert the compressed column $\partial_j$ in $R$ as the new 
$j$-th column.  If, due to the addition of $\partial_j$, $R$ becomes unreduced (\emph{i.e.}, if the 
pivots of the columns of $R$ are no longer in unique rows), scan the columns from $j$ to $n$, and 
reduce them: while the column $k \geq j$ is non-empty, and its pivot is the pivot of another column 
$l < k$, add column $l$ to column $k$.


Let $R^\prime$ be the reduction returned by the above algorithm.  If the simplex $\sigma$ was 
present in the filtration $K$ to begin with, the reduction obtained from that filtration is denoted 
by $R_{org}$.


\begin{theorem}\label{th1}
	$R^\prime$ and $R_{org}$ produce the same persistence intervals.
\end{theorem}


\begin{proof}
	It has already been established that the compression step does not alter the pivots of a 
	reduction \cite{kerber-18, bauer-14-clear}.  Since the columns are reduced by left-to-right 
	column additions, the insertion of a new $j$-th column in $R$ does not impact any column $i < 
	j$.
	
	As the new simplex $\sigma$ is added to the filtration $K$, an existing simplex $\tau \in K$ 
	can become a facet of $\sigma$.  However, $\tau$ can never be a cofacet\footnote{A cofacet is a 
	co-dimension one coface of a simplex.} of $\sigma$.  In other words, an existing simplex of $K$ 
	can be a facet of a new simplex, but the new simplex can not be a facet of an existing 
	simplex.  This is due to the definition of the simplicial complex $K$: every face of an 
	existing simplex $\tau$ must already be contained in $K$.  It means that no column $k > j$ can 
	have a non-zero entry in the $j$-th row.
	
	
	Since any reduced column is a \emph{linear combination of itself and the columns to its left}, 
	the order of column operations does not alter the reduction of a column.
	
	Therefore, $R^\prime$ 
	and $R_{org}$ have the same pivots and produce the same persistence intervals.
	
	
\end{proof}



\subsection{Algorithm 2}

This algorithm represents a more time-efficient version of Algorithm $1$.  In particular, the 
addition and reduction of one simplex at a time might not result in an optimal implementation.  As 
such, we formulate a variant of Algorithm $1$ based on the strategy of the reduction of 
chunks as suggested in \cite{kerber-18}.  Instead of adding one column at a time, we insert a chunk 
of $C$ columns at a time into $R$.  An additional advantage of inserting columns in chunks is that 
we can exploit the popular \emph{clearing optimization} strategy.  When a new data point arrives 
from the stream, we build all the simplices around the new point using an iterative algorithm 
\cite{zomorodian-10}.  Therefore, in a streaming environment, every new data point naturally 
creates a chunk of simplices.  Even though the chunk size $C$ represents a trade-off between time 
and space efficiency, Kerber \emph{et al.} \cite{kerber-18} recommend that one should choose a 
large value of $C$ to rip higher benefits from the clearing optimization.  In a real streaming 
application, we can adjust $C$ based on the stream speed (the number of incoming data points per 
time unit) and the update frequency of the persistence intervals required by the application.  The 
algorithm is described below.


We insert $C$ columns into $R$ at a time, and compress them as before.  Let $j$ be the lowest index 
of the newly inserted columns.  Then, we start the reduction of the columns $j$, $j + 1$, ..., $n$ 
using the clearing optimization.  That is, we reduce the columns $j$, $j + 1$, ..., $n$ in 
decreasing dimension and set a column to zero as soon as its index becomes the pivot of another 
column \cite{chen-11}.


\begin{theorem}
	The above algorithm computes the correct persistence intervals.
\end{theorem}


\begin{proof}
	
	The clearing optimization has already been shown to produce the same reduction as the standard
	algorithm.  In Theorem \ref{th1}, we have shown that incrementally adding and reducing columns  
	do not alter the persistence intervals.  Therefore, Algorithm $2$ as described above produces 
	the correct persistence intervals.
	
\end{proof}



\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}