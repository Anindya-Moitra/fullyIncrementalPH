\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{times}
\usepackage{cite}
\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsfonts}
%\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{url}
\usepackage{amsthm}
% \usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\usepackage{color}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{relsize}


\author{Anindya Moitra}

\title{Incremental Matrix Reduction}
\date{\vspace{-5ex}}


\begin{document}
	

\maketitle

\section{The Problem}

This document addresses one of the primary challenges of computing persistent homology on streaming 
data by a fully incremental approach.  As a data stream is a potentially infinite sequence of data 
objects, the entire stream cannot be stored in the memory typically available to a computer.  
Therefore, the computation of persistent homology on streaming data requires an incremental 
approach.  A couple of computational models for applying persistent homology on data streams are 
being developed as two separate projects that involve partially incremental approaches.  In 
particular, consistent with the standard computational paradigm \cite{silva-13} for processing 
data streams, each of those two models consists of two principal components: (i) \emph{online}, and 
(ii) \emph{offline}.  However, yet another (\emph{i.e.}, a third) computational model can be 
developed by a \emph{fully online} or \emph{fully incremental} approach.


A key requirement for developing such a fully incremental model for persistent homology would be 
the ability to perform the \emph{Gaussian elimination} (also called the \emph{reduction}) of the 
boundary matrix \cite{edelsbrunner-00, zomorodian-05} by an incremental algorithm.  The Gaussian 
elimination step is performed during the offline component (\emph{i.e.}, as a \emph{batch 
processing} mechanism) in the previous two models for computing persistent homology on streaming 
data.  This document attempts to develop the theoretical foundation for performing the 
\emph{Gaussian elimination} by an incremental algorithm.


The standard algorithm \cite{edelsbrunner-00, zomorodian-05} \emph{reduces} the boundary matrix 
$\partial$ of a filtration \cite{zomorodian-10} to the column-echelon form $R$.  Usually, the 
entire boundary matrix or a simplified data structure thereof is processed in the memory while the 
standard algorithm is executed.  This approach is not desirable when computing persistent homology 
on streaming data by a fully incremental mechanism.  When working with data streams processed by a 
fully incremental model, one would want to add a simplex $\sigma$ to the already reduced matrix $R$ 
without having to recompute the reduction of other columns due to the addition of $\sigma$.  
Ideally, only the column of $\sigma$ should be reduced as the simplex $\sigma$ is added to the 
filtration.

It is worth mentioning that while computing persistent homology on data streams is one of the 
primary target applications of the incremental Gaussian elimination algorithm, it will have other 
important applications as well.  For example, due to the large size of the \emph{complex} 
constructed on a point cloud, the dimension of the boundary matrix increases exponentially with the 
number of data objects on which persistent homology is being computed.  As a result, reducing the 
boundary matrix by a batch processing algorithm becomes prohibitive even for `static' (\emph{i.e.}, 
non-streaming) data sets of moderate size (such as, data sets with up to a few thousands of 
objects, depending on the memory available to the computer).  The incremental algorithm developed 
by this project will help in the Gaussian elimination of the boundary matrix where this is 
not possible by batch processing mechanisms due to the size of the matrix.


Kerber \textit{et al.} \cite{kerber-18} introduced a streaming algorithm for reducing the boundary 
matrix based on optimized versions of the standard algorithm \cite{edelsbrunner-00, 
zomorodian-05}.  However, their algorithm assumes that the entire data set or filtration is 
available on disk.  Therefore, the total ordering of the simplices is predetermined.  The next 
simplex added to $R$ has a higher weight than any of the previously added simplices.


In a real streaming application, the entire point cloud or filtration is not available at any 
time.  Hence, every time a new simplex $\sigma$ is added to the filtration, the indices of those 
simplices that have weights higher than that of $\sigma$ are incremented by one.  In other words, 
in a real streaming environment, new simplices do not arrive from a sequence of simplices sorted 
according to their increasing weights.  Therefore, the algorithm of \cite{kerber-18} can not be 
applied to real-world streaming applications.



In this document, we prove that a streaming or incremental version of the standard algorithm 
exists.  In particular, we prove that even when a simplex $\sigma$ that changes the 
indices of other simplices is added to an already reduced matrix $R$, it is possible to only reduce 
the column of $\sigma$ without recomputing the reduction of other columns.



\section{A Solution}

\subsection{Incremental Matrix Reduction Algorithm}

An incremental variant of the standard algorithm can be described as follows.  Let us assume that a 
boundary matrix $\partial$ is already reduced to the column-echelon form $R$.  When a new simplex 
$\sigma$ arrives, we compute its index $j$ with respect to the existing filtration.  We also 
compute the row $r$ and the \emph{unreduced} column $c$ of $\sigma$. $c$ specifies the 
facets\footnote{A facet is a co-dimension one face of a simplex.} of $\sigma$, while $r$ contains 
only zeroes.

\begin{theorem}
	The algorithm described above computes the correct barcodes.
\end{theorem}



\begin{proof}
	Assume that the new simplex $\sigma$ is added to an existing filtered simplicial complex $K$.  
	An existing simplex $\tau \in K$ can become a facet of $\sigma$.  However, $\tau$ can never be 
	a cofacet\footnote{A cofacet is a co-dimension one coface of a simplex.} of $\sigma$.  In other 
	words, an existing simplex of $K$ can be a facet of a new simplex, but the new simplex can not 
	be a facet of an existing simplex.  This is due to the definition of the simplicial complex 
	$K$: every face of an existing simplex $\tau$ must already be contained in $K$.
	
	The $j$-th column of the boundary matrix $\partial$ encodes the facets of the simplex 
	$\sigma_j$, and the $i$-th row of $\partial$ encodes the cofacets of $\sigma_i$.  Since the new 
	simplex $\sigma$ can not have an existing cofacet, $r$ contains only zeroes.
	
	
	The standard algorithm iterates over the columns of the boundary matrix $\partial$ from left to 
	right, reducing the columns by \emph{left-to-right column additions}.  Therefore, the newly 
	added $j$-th column $c$ needs to be reduced according to the standard column reduction 
	procedure described above.
	
	
	We call $R$ reduced when all the pivots are in unique rows.  It is well-known that, although 
	$\partial$ does not have a unique reduction, the pivots of all its reductions are the same.  
	When a new data point arrives from the stream, the corresponding new simplices are created by 
	iteratively adding the cofaces upto the maximum dimension $k$.  The data point itself creates a 
	0-simplex.  When that 0-simplex is added to $R$, it has no effect on the existing persistence 
	intervals.  This is because the column of a 0-simplex contains only zeroes.  The addition of 
	the subsequent cofaces of the 0-simplex can not introduce a pivot that equals the pivot of a 
	higher column in $R$.
	
\end{proof}


\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}