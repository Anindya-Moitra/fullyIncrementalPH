\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{times}
\usepackage{cite}
\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsfonts}
%\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{url}
\usepackage{amsthm}
% \usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\usepackage{color}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{relsize}


\author{Anindya Moitra}

\title{Incremental Matrix Reduction}
\date{\vspace{-5ex}}


\begin{document}
	

\maketitle

\section{The Background}

This document addresses one of the primary challenges of computing persistent homology on streaming 
data by a fully incremental approach.  As a data stream is a potentially infinite sequence of data 
objects, the entire stream cannot be stored in the memory typically available to a computer.  
Therefore, the computation of persistent homology on streaming data requires an incremental 
approach.  A couple of computational models for applying persistent homology on data streams are 
being developed as two separate projects that involve partially incremental approaches.  In 
particular, consistent with the standard computational paradigm \cite{silva-13} for processing 
data streams, each of those two models consists of two principal components: (i) \emph{online}, and 
(ii) \emph{offline}.  However, yet another (\emph{i.e.}, a third) computational model is developed 
by a \emph{fully online} or \emph{fully incremental} approach in this project.


A key requirement for developing such a fully incremental model for persistent homology is the 
ability to perform the \emph{Gaussian elimination} (also called the \emph{reduction}) of the 
boundary matrix \cite{edelsbrunner-00, zomorodian-05} by an incremental algorithm.  The Gaussian 
elimination step is performed during the offline component (\emph{i.e.}, as a \emph{batch 
processing} mechanism) in the previous two models for computing persistent homology on streaming 
data.  This document develops the theoretical foundation for performing the \emph{Gaussian 
elimination} by an incremental algorithm.


It is worth mentioning that while computing persistent homology on data streams is one of the 
primary target applications of the incremental Gaussian elimination algorithm, it will have other 
important applications as well.  For example, due to the large size of the \emph{complex} 
constructed on a point cloud, the dimension of the boundary matrix increases exponentially with the 
number of data objects on which persistent homology is being computed.  As a result, reducing the 
boundary matrix by a batch processing algorithm becomes prohibitive even for `static' (\emph{i.e.}, 
non-streaming) data sets of moderate size (such as, data sets with up to a few thousands of 
objects, depending on the memory available to the computer).  The incremental algorithm developed 
by this project will help in the Gaussian elimination of the boundary matrix where this is 
not possible by batch processing mechanisms due to the size of the matrix.




\section{The Problem}


The standard algorithm \cite{edelsbrunner-00, zomorodian-05}, as described in 
\cite{edelsbrunner-10, otter-17, chen-11, kerber-18} among others, computes the persistence of a 
filtration \cite{zomorodian-10} by \emph{reducing} its boundary matrix $\partial$ to a 
column-echelon form $R$.  Usually, the entire boundary matrix or a simplified data structure 
thereof is processed in the memory while the algorithm is executed.  This approach is not desirable 
when computing persistent homology on streaming data by a fully incremental mechanism.  When 
working with data streams processed by a fully incremental model, one would want to add a simplex 
$\sigma$ to the already reduced matrix $R$ without having to recompute the reduction of all other 
columns due to the addition of $\sigma$.


Kerber \emph{et al.} \cite{kerber-18} introduced a streaming algorithm for reducing the boundary 
matrix based on optimized versions of the standard algorithm \cite{edelsbrunner-00, 
zomorodian-05}.  However, their algorithm assumes that the entire data set or filtration is 
available on disk.  Therefore, the total ordering of all the simplices in the filtration is known 
\emph{a priori}.  The next simplex added to $R$ has a higher weight than any of the previously 
added simplices.


In a real streaming application, the entire point cloud or filtration is not available at any 
time.  Hence, every time a new simplex $\sigma$ is added to the filtration, the indices of those 
simplices that have weights higher than that of $\sigma$ are incremented by one.  In other words, 
in a real streaming environment, the column corresponding to a new simplex does not necessarily get 
added to the right of $R$.  Therefore, the algorithm of \cite{kerber-18} can not be applied to 
real-world streaming applications.


In the next section, we provide two algorithms to incrementally reduce a boundary matrix after the 
addition of simplices.  The goal of both of the algorithms is to compute the updated reduction of 
the boundary matrix $\partial$ when new columns are added to $R$, the existing reduction of 
$\partial$.



\section{Addition of Simplices: Two Solutions}

\subsection{Algorithm 1}

Let us assume that $R$ is the reduction of a boundary matrix $\partial$ associated to a filtration 
$K$ that has $n$ simplices.  When a new simplex $\sigma$ needs to be added to $R$, we compute the 
index $j$ and the unreduced column $\partial_j$ of $\sigma$ with respect to the total ordering of 
the filtration.  $\partial_j$ specifies the facets\footnote{A facet is a co-dimension one face of a 
simplex.} of $\sigma$.

We then apply an optimization technique called \emph{compression} \cite{kerber-18} on the column 
$\partial_j$: scan through the non-zero entries of $\partial_j$; If a row index $i$ corresponds to 
a negative simplex (\emph{i.e.}, if the $i$-th column of $R$ is not zero), remove the non-zero entry
from the column $\partial_j$.  We then insert the compressed column $\partial_j$ in $R$ as the new 
$j$-th column.  If, due to the addition of $\partial_j$, $R$ becomes unreduced (\emph{i.e.}, if the 
pivots of the columns of $R$ are no longer in unique rows), scan the columns from $j$ to $n$, and 
reduce them: while the column $k \geq j$ is non-empty, and its pivot is the pivot of another column 
$l < k$, add column $l$ to column $k$.


Let $R\prime$ be the reduction returned by the above algorithm.  If the simplex $\sigma$ was 
present in the filtration $K$ to begin with, the reduction obtained from that filtration is denoted 
by $R_{org}$.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}